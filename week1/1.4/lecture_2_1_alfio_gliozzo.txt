[ MUSIC ]

GLIOZZO:	This lesson is the second and it might be also the most important of the entire course, because here we will talk about the architecture that has been used by the DeepQA team to build the Jeopardy! system.  So, and it is, in my opinion, the most brilliant invention in the QA literature in the last decades, I believe.

So, the reason because Watson works is because the DeepQA Architecture has been elaborated in a way that is very flexible and it allows the integration of a variety of different technologies including machine learning, natural language processing and knowledge representation reasoning, and almost everything you can think about in the artificial intelligence field.  So, it will be a two-hour lesson, so I will have a break for questions after one hour.  Okay, let's get started.

So, this is the Watson architecture from a very high level perspective.  So, as you can see, it's pretty massive, so it means that there is a lot of things going on.  I will try to describe those small blocks one by one functionally, so in a way that you can understand what they are supposed to do.

And first of all, what is very, very and maybe the most important innovation in the DeepQA Architecture is the fact that we can use simultaneously both structured and unstructured data in the same coherent framework.  So, structured data that are represented with these small ontology fragments and unstructured data that are mostly text corpora derived from the Web or from Wikipedia or from different sources.

They're used together in combination as a way to get new possible candidate answers.  So, Watson uses both simultaneously to derive possible answers.  But those answers could be eventually wrong, so the way Watson scores the answer and tries to figure out what is the correct one is to look for additional evidence that comes, again, from both unstructured and structured data sources.

So, those, the same two sources are then used for both looking for candidate answers and looking for evidence supporting those answers.  So, this just the input data that Watson needs to answer a question.  Under the hood, there is a variety of different methods and technologies to make the best use of this data.  After realizing the occurrences of those answers in text is kind of able to judge whether the answer is correct or not in respect to the question.

So, this is a very, very high-level description of what's going on.  So, if you think about that here, what is challenging in building an application like that?  First of all, you need to have experts in the area of natural language processing working together with machine learning people, which is not that unusual.

But what is really unusual is to work together with people expert in knowledge base and Semantic Web and knowledge representation.  Traditionally, those two communities were not partnering each other in a way that is needed to build an architecture like that.  So, one of the innovations of Watson is to put together, let's say, symbolic methods and machine learning based methods in a coherent architecture that makes use of both.

So, the system integration and semantic analysis and integration component of this architecture is very important.  So, what I'm going to do in this lesson is to describe not everything here, because it's going to be too much.  And but just describe a simplified version of this DeepQA architecture that is what we call the minimum DeepQA pipeline, which is what is needed to get answers from natural language questions of a specific type of question that are the factoid questions.

The input is supposed to be text.  Of course, if you had audio or other media, first you have to convert in text.  So, once the question is ingested by Watson, the first task is to analyze the content of this question, which is mostly a natural language processing type of problem that once you have a good understanding of the question in terms of the entity involved there, the relations, the possible categories of the answer that they're looking for, then Watson does information retrieval type of operation.

That is called "primary search."  So, what is the goal of primary search is to come up with a set of possible sources that come from either structured and unstructured data that eventually helps you in identifying the possible candidate answers that are the hypotheses that Watson then evaluates to come up with the right one.

So, the result of primary search is analyze like with some type of information abstraction algorithm that is able to identify entities inside text, and hypotheses are generated.  So, after this step, you are going to find a list of entities -- strings, basically -- describing possible locations, persons, but any possible type is going to be a possible hypothesis generated by Watson.

So, after this, the next step is to score the evidence for each of these hypotheses.  If you think about this problem here, is kind of very time consuming because this job should be done not only once for a question but once per possible candidate answer.  And the idea behind Watson is that Watson is expanding, is generating hundreds of different possible candidates to have a very large recall.

So, and in that set, Watson needs to find the right one.  So, these step will be executed in every possible candidate answer, so that's time consuming and expensive.  That's why you need to run it in parallel, otherwise it would never end.
So, once you have features describing the quality of those answers in different ways and from multiple points of view, the final task is to put them together and try to figure out what are the features that really matter to decide what is the right answer in Watson.  And that's a machine learning type of problem, so it's kind of learning how to weigh the impact of this feature with respect to different question classes or different questions in general.

So, this is the pipeline I'm going to describe in the rest of this lesson here.  So, let me start with an example so we can see how it works in real life.  So, if you take this question here, "In 1894, C.W. Post created his warm cereal drink Postum in this Michigan city." That's one of the typical Jeopardy! questions that can be input to Watson.

So, the first step is question analysis: what is the result of the question analysis?  As you can see here, is a very traditional, if you wish, information extraction problem where we are looking for possible keywords.  So, terms that are highly discriminating the content of this question as opposed to terms that are just syntactic, syntactic functions, they are not important if you are looking for extra information about this question.

So, what we want is to define the type of the answer that Watson is supposed to generate.  So, in this case, we are looking for answers of type Michigan city.  So, we call it lexical answer type, because it was not possible for us to come up with an ontology of types that is broad enough to describe the entire variety of different answers in Jeopardy!, so we decided to use the language itself as an ontology.

So, after we identify the lexical answer type, we may be interested in some type of more deep semantic analysis of the context of the content in the question itself.  So, and usually what we want to do is kind of an information abstraction task, where you find information of interest for these questions.

So, the information abstraction approach used by Watson is very, let's say, open domain.  So, we are not restricting our analytics to identify just a very small set of types but we are open to handle many different types.  And the way we do that is to put together different analytics doing different things.  Anyway, but I will go into details of this later in the class.

So, once you have done with the question analysis, and then the next step for Watson is to perform a primary search, which is not very different than what we can do if you type the keywords like these terms here, in a standard search engine like Lucene.

But Watson is also looking for information in databases.  So, from this perspective, it takes the primary search operation not very standard, because it's not just searching for the keywords in the document, but is also searching for kind of logical form, expression derived from the question, is using that expression as a query for knowledge bases [often with] the sources in the right place, so we identify eventual documents that can be eventually in the order of several dozens, 100 or less, and we have also a bunch of [UNI] in a knowledge base that we can use as...

What we need is to identify possible candidate answers.  It turns out that candidate answers are represented as strings, so we're just identifying nouns from phrases there, or it could be eventually not only noun phrases but we're identifying entities that matters from this content.

So, of course, most of this answer could be wrong because actually in the Jeopardy! game, only one answer is supposed to be correct.  So even though we are going to find like 400 candidates, and just one is the right one.  How do you get, how do you find the right answer?

So, what Watson does is to look for additional evidence for any of these candidates.  So, additional evidence can come from the corpus itself; and in that case, could be a search query -- for example, for the term "General Foods against the same index that was used for primary search, but now we are looking for candidate answers.

And it turns out that "General Foods" is not the case.  Why?  Because, let's take a look at this feature.  Watson has many different ways to understand whether this answer is correct or not.  And we call them answer scoring features.  And every answer scoring feature is generated by [lack] of a small annotator in UIMA [INAUDIBLE] problem, something that analyzes this question here to its answer here together with the evidence and analyze the question, and together and try to compare from different perspectives.

For example, one perspective is the lexical overlap.  So, how many keywords are in common between this, a simple question here and the supporting evidence of this question?  That could be one very easy way to rank them.  But of course, it's not going to be very effective.  Why?  Because this operation of lexical overlap has been done before in the candidate generation, so it's not going to be a very important new information.

But we can also look for other types of information.  And one of them can be taxonomic relations between this entity here and the lexical answer type.  So, f you have the word "General Foods" and we ask...and the Watson asks himself, is General Foods of type Michigan City?  The result would be, no.  So, we give a zero to this feature.

But for other entities here you may find cities.  And for example, Battle Creek and Grand Rapids are city in nature, so in this case, you have a very good score for this taxonomic evidence.  So, this is what we call TyCor, Type Coercion.

So, let's look at other features that could be the special type of reasoning.  So, you may ask yourself, and Watson is asking himself, if General Foods is somehow related from a geographic perspective to the question.  And it looks like that in the question there is a good suggestion that you are looking for something in Michigan.

So, this feature here is going to evaluate the proximity geographically between the entities and Michigan.  And it turns out that it can be done sometimes not for everything, for example, for a date I don't know if it makes sense to do this type of comparison.  But for something like a city, of course, it makes a lot of sense.

But maybe also for a company, because it can be based in Michigan.  So, following this type of reasoning you may find very important suggestions from this type of analytics.  Watson is actually using hundreds of analytics, so one of them is also a temporal type of feature.  So, let's look at this one.

So, being able to derive all these features in the proper way is most of the hard work that the Watson team did after we finalized the right architecture, because with an architecture like that, adding new features is...adding a new component that is looking at the same information, so question and answer together, and derive a different point of view.  So, this is most of the work of developing Watson and making it...and increasing its performance.

So, what Watson does with the feature is to combine them together at the very end and using them as features vectors representing each candidate.  So, the candidate General Foods is described by this vector; Battle Creek is described by another vector.

So, a linear combination...actually, not a linear combination but a realistic relation function is used by Watson to decide what is the target confidence.  And the confidence is kind of a combination of this feature weighted according to some learning that has been done from historical data.

Historical data are derived from the historical competition.  As I said last lesson, the Jeopardy! is on the air from 40 years, so there is an archive called J! Archive that is basically collecting the historical data containing question and answer in the last 40 years of games, and so we can train it from that.  I will show you how it works later.

Okay, now that I presented this pipeline with an example, now I want to come back again to the same pipeline and try to expand any of this block to see more detail on how it works on the [INAUDIBLE].

So, let's say again with the question analysis.  So, here is an example, and actually answer your question before.  So, if you have a question like, poets and poetry.  He was a bank clerk in the Yukon before he published Songs of a Sourdough in 1907.

So, there are lexical answer types here and this could be a poet, it can be "he," because "he" itself is a type, is a male person.  It could be "clerk."  So, this is part of the work going on in question analysis, to consider every word and every combination of more than one word and try to decide, what is a lexical answer type and what is not.

It can be done in different ways.  We go in the natural language processing lessons and we'll describe how it is done.  You can think about it as information extraction problem, going to classify these words as lexical answer type or not.  But you can think about other solutions, but everything that works is supposed to derive a list of words.
[END OF SEGMENT]
